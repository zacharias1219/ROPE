{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROPE Demo on Google Colab\n",
        "\n",
        "Run the ROPE demo on a Colab GPU and download the results.\n",
        "\n",
        "**Before starting:**\n",
        "1. **Runtime → Change runtime type → GPU** (T4 or A100).\n",
        "2. Get a [HuggingFace token](https://huggingface.co/settings/tokens) if you want to use Llama (phi2 works without it).\n",
        "\n",
        "**Option A:** You have the ROPE repo on GitHub → set `REPO_URL` below and run the clone cell.\n",
        "\n",
        "**Option B:** No GitHub → zip your local ROPE folder, upload it in cell 2, then run the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install dependencies and clone or use uploaded ROPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "/content/ROPE\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for rope-bench (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "ROPE installed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU. Set Runtime → Change runtime type → GPU.\")\n",
        "\n",
        "# Option A: Clone from GitHub (set your repo URL)\n",
        "REPO_URL = \"https://github.com/zacharias1219/ROPE.git\"  # Change to your repo, or leave and use Option B\n",
        "REPO_DIR = \"ROPE\"\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    if \"yourusername\" in REPO_URL:\n",
        "        print(\"Set REPO_URL to your repo, or upload a zip of the ROPE folder (Option B below).\")\n",
        "    else:\n",
        "        !git clone --depth 1 \"{REPO_URL}\" \"{REPO_DIR}\"\n",
        "\n",
        "if os.path.isdir(REPO_DIR):\n",
        "    %cd {REPO_DIR}\n",
        "    !pip install -e . -q\n",
        "    print(\"ROPE installed.\")\n",
        "else:\n",
        "    print(\"ROPE folder not found. Upload a zip in the next cell (Option B).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Option B: Upload ROPE zip (if you didn't clone)\n",
        "\n",
        "Zip your local `ROPE` project folder (include `rope/`, `data/`, `pyproject.toml`, etc.), upload below, then re-run cell 1 logic in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this only if you're uploading a zip. Upload the zip when prompted.\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()  # Opens file picker\n",
        "for name in uploaded:\n",
        "    if name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(name, 'r') as z:\n",
        "            z.extractall('.')\n",
        "        print(f\"Extracted {name}\")\n",
        "        break\n",
        "# If zip had a single root folder (e.g. ROPE/), we're in that folder now or in content.\n",
        "# Go to the folder that contains pyproject.toml\n",
        "import os\n",
        "for d in ['ROPE', '.', 'rope-bench']:\n",
        "    if os.path.isfile(os.path.join(d, 'pyproject.toml')):\n",
        "        %cd {d}\n",
        "        !pip install -e . -q\n",
        "        print(f\"Installed from {d}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HuggingFace login (for Llama models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login successful. Gated models (Llama) will now work.\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = getpass(\"Paste your HuggingFace token (or press Enter to skip; phi2 works without it): \")\n",
        "if token.strip():\n",
        "    login(token=token.strip())\n",
        "    print(\"Login successful. Gated models (Llama) will now work.\")\n",
        "else:\n",
        "    print(\"Skipped. Use phi2-only runs (e.g. rope run --models phi2) or add token for Llama.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run ROPE demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running ROPE demo...\n",
            "\n",
            "Loading tasks and attacks...\n",
            "  Loaded 30 tasks, 20 attacks\n",
            "\n",
            "Loading judge model (llama3-8b)...\n",
            "Loading llama3-8b (meta-llama/Meta-Llama-3-8B-Instruct)...\n",
            "config.json: 100% 654/654 [00:00<00:00, 2.88MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 5.17MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...):  99% 15.9G/16.1G [00:53<00:01, 194MB/s]  \n",
            "Fetching 4 files: 100% 4/4 [00:53<00:00, 13.44s/it]\u001b[A\n",
            "Download complete: 100% 16.1G/16.1G [00:53<00:00, 298MB/s]                \n",
            "Loading weights: 100% 291/291 [00:05<00:00, 56.35it/s, Materializing param=model.norm.weight]                              \n",
            "generation_config.json: 100% 187/187 [00:00<00:00, 1.04MB/s]\n",
            "tokenizer_config.json: 100% 51.0k/51.0k [00:00<00:00, 4.77MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 11.7MB/s]\n",
            "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 435kB/s]\n",
            "  Loaded llama3-8b on cuda:0\n",
            "\n",
            "============================================================\n",
            "EVALUATING: llama2-7b\n",
            "============================================================\n",
            "Loading llama2-7b (meta-llama/Llama-2-7b-chat-hf)...\n",
            "config.json: 100% 614/614 [00:00<00:00, 3.28MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 90.2MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...):  99% 13.4G/13.5G [00:45<00:00, 540MB/s]   \n",
            "Fetching 2 files: 100% 2/2 [00:46<00:00, 23.08s/it]\u001b[A\n",
            "Download complete: 100% 13.5G/13.5G [00:46<00:00, 292MB/s]                \n",
            "Loading weights: 100% 291/291 [00:03<00:00, 76.14it/s, Materializing param=model.norm.weight]                               n.v_proj.weight]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.02MB/s]\n",
            "tokenizer_config.json: 100% 1.62k/1.62k [00:00<00:00, 9.04MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 3.03MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 975kB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.59MB/s]\n",
            "  Loaded llama2-7b on cuda:0\n",
            "\n",
            "  Defense: none\n",
            "  llama2-7b/none: 100%|█████████████████████████| 20/20 [04:43<00:00, 14.20s/it]\n",
            "\n",
            "  Defense: delimiter\n",
            "  llama2-7b/delimiter: 100%|████████████████████| 20/20 [01:24<00:00,  4.24s/it]\n",
            "\n",
            "============================================================\n",
            "Saving results to demo_results.json...\n",
            "  Saved 40 results\n",
            "============================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ROPE EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Model: llama2-7b\n",
            "  Defense: delimiter\n",
            "  ASR (any success): 0.0% [LOW RISK]\n",
            "  ASR (complete hijack): 0.0%\n",
            "  Avg Severity: 0.00/3.0\n",
            "\n",
            "Model: llama2-7b\n",
            "  Defense: none\n",
            "  ASR (any success): 0.0% [LOW RISK]\n",
            "  ASR (complete hijack): 0.0%\n",
            "  Avg Severity: 0.00/3.0\n",
            "\n",
            "======================================================================\n",
            "  Best defense for llama2-7b: delimiter (ASR: 0.0%)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Demo complete! Run full evaluation with: rope run\n"
          ]
        }
      ],
      "source": [
        "# Standard demo: llama2-7b + 2 defenses + 20 attacks (~5–15 min on T4)\n",
        "!rope demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. View results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'demo_results_metrics.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3385526517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"demo_results_metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demo_results_metrics.csv'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "with open(\"demo_results.json\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Try to load CSV, or compute from JSON if missing\n",
        "if os.path.exists(\"demo_results_metrics.csv\"):\n",
        "    metrics = pd.read_csv(\"demo_results_metrics.csv\")\n",
        "else:\n",
        "    # Compute metrics from JSON (for older rope demo runs)\n",
        "    from rope.metrics import compute_metrics\n",
        "    metrics = compute_metrics(results)\n",
        "    print(\"Note: CSV not found, computed metrics from JSON.\")\n",
        "\n",
        "print(\"Summary metrics:\")\n",
        "display(metrics)\n",
        "print(f\"\\nTotal evaluations: {len(results)}\")\n",
        "print(\"\\nSample result:\")\n",
        "print(json.dumps(results[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Download result files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "for name in [\"demo_results.json\", \"demo_results_metrics.csv\", \"demo_results_report.txt\"]:\n",
        "    if os.path.exists(name):\n",
        "        files.download(name)\n",
        "        print(f\"Downloaded {name}\")\n",
        "    else:\n",
        "        print(f\"Not found: {name}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
