{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROPE Results Analysis\n",
        "\n",
        "This notebook provides tools for analyzing ROPE evaluation results,\n",
        "creating visualizations, and identifying patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load results (adjust path as needed)\n",
        "RESULTS_PATH = \"../results.json\"  # Change to your results file\n",
        "\n",
        "try:\n",
        "    with open(RESULTS_PATH) as f:\n",
        "        results = json.load(f)\n",
        "    print(f\"Loaded {len(results)} results\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Results file not found at {RESULTS_PATH}\")\n",
        "    print(\"Run an evaluation first: rope run --models phi2 --defenses none,delimiter\")\n",
        "    # Create synthetic results for demonstration\n",
        "    np.random.seed(42)\n",
        "    results = []\n",
        "    for model in [\"llama2-7b\", \"llama3-8b\", \"phi2\"]:\n",
        "        for defense in [\"none\", \"delimiter\", \"icl\"]:\n",
        "            for attack_type in [\"hijack\", \"extract\", \"obfuscate\", \"poison\"]:\n",
        "                for task_family in [\"qa\", \"summarize\", \"rag\"]:\n",
        "                    for i in range(10):\n",
        "                        base = {\"none\": 1.5, \"delimiter\": 0.8, \"icl\": 0.6}[defense]\n",
        "                        severity = min(3, max(0, int(np.random.normal(base, 1))))\n",
        "                        results.append({\n",
        "                            \"model\": model,\n",
        "                            \"defense\": defense,\n",
        "                            \"task_id\": i + 1,\n",
        "                            \"task_family\": task_family,\n",
        "                            \"attack_type\": attack_type,\n",
        "                            \"severity\": severity,\n",
        "                            \"response\": \"synthetic\"\n",
        "                        })\n",
        "    print(f\"Created {len(results)} synthetic results for demonstration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "print(f\"Models: {df['model'].unique()}\")\n",
        "print(f\"Defenses: {df['defense'].unique()}\")\n",
        "print(f\"Attack types: {df['attack_type'].unique()}\")\n",
        "print(f\"Task families: {df['task_family'].unique()}\")\n",
        "print(f\"\\nSeverity distribution:\")\n",
        "print(df['severity'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Overall Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rope.metrics import compute_metrics, compute_by_attack_type, compute_by_task_family, print_summary\n",
        "\n",
        "# Overall metrics\n",
        "metrics = compute_metrics(results)\n",
        "print_summary(metrics)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Defense Comparison Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Chart 1: ASR by model and defense\n",
        "ax = axes[0]\n",
        "pivot = metrics.pivot(index='defense', columns='model', values='asr_1plus')\n",
        "pivot.plot(kind='bar', ax=ax, width=0.7)\n",
        "ax.set_title('Attack Success Rate by Model and Defense')\n",
        "ax.set_ylabel('ASR (severity >= 1)')\n",
        "ax.set_xlabel('Defense')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend(title='Model')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
        "\n",
        "# Chart 2: Average severity\n",
        "ax = axes[1]\n",
        "pivot_sev = metrics.pivot(index='defense', columns='model', values='avg_severity')\n",
        "pivot_sev.plot(kind='bar', ax=ax, width=0.7)\n",
        "ax.set_title('Average Severity by Model and Defense')\n",
        "ax.set_ylabel('Average Severity (0-3)')\n",
        "ax.set_xlabel('Defense')\n",
        "ax.set_ylim(0, 3)\n",
        "ax.legend(title='Model')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('defense_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('Saved defense_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Attack Type Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    print(\"Install seaborn: pip install seaborn\")\n",
        "    sns = None\n",
        "\n",
        "if sns:\n",
        "    by_type = compute_by_attack_type(results)\n",
        "\n",
        "    # Create one heatmap per model\n",
        "    models = by_type['model'].unique()\n",
        "    fig, axes = plt.subplots(1, len(models), figsize=(6 * len(models), 4))\n",
        "    if len(models) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, model in zip(axes, models):\n",
        "        model_data = by_type[by_type['model'] == model]\n",
        "        pivot = model_data.pivot_table(\n",
        "            values='asr_1plus',\n",
        "            index='defense',\n",
        "            columns='attack_type'\n",
        "        )\n",
        "        sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn_r',\n",
        "                    vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'ASR'})\n",
        "        ax.set_title(f'{model}: ASR by Defense x Attack Type')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attack_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print('Saved attack_heatmap.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Task Family Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "by_family = compute_by_task_family(results)\n",
        "print(\"Metrics by task family:\")\n",
        "by_family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task family vulnerability comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Filter to 'none' defense to show raw vulnerability\n",
        "none_family = by_family[by_family['defense'] == 'none'] if 'none' in by_family['defense'].values else by_family\n",
        "pivot_fam = none_family.pivot_table(values='asr_1plus', index='task_family', columns='model')\n",
        "pivot_fam.plot(kind='bar', ax=ax, width=0.7)\n",
        "ax.set_title('Vulnerability by Task Family (No Defense)')\n",
        "ax.set_ylabel('ASR (severity >= 1)')\n",
        "ax.set_xlabel('Task Family')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend(title='Model')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Finding 1: Most/least robust model\n",
        "none_metrics = metrics[metrics['defense'] == 'none'] if 'none' in metrics['defense'].values else metrics\n",
        "if len(none_metrics) > 0:\n",
        "    most_robust = none_metrics.loc[none_metrics['asr_1plus'].idxmin()]\n",
        "    least_robust = none_metrics.loc[none_metrics['asr_1plus'].idxmax()]\n",
        "    print(f\"\\n1. Most robust model (no defense): {most_robust['model']} (ASR: {most_robust['asr_1plus']:.1%})\")\n",
        "    print(f\"   Least robust model (no defense): {least_robust['model']} (ASR: {least_robust['asr_1plus']:.1%})\")\n",
        "\n",
        "# Finding 2: Best defense overall\n",
        "defense_avg = metrics.groupby('defense')['asr_1plus'].mean()\n",
        "best_defense = defense_avg.idxmin()\n",
        "print(f\"\\n2. Best defense overall: {best_defense} (avg ASR: {defense_avg[best_defense]:.1%})\")\n",
        "\n",
        "# Finding 3: Hardest attack type\n",
        "by_type = compute_by_attack_type(results)\n",
        "type_avg = by_type.groupby('attack_type')['asr_1plus'].mean()\n",
        "hardest = type_avg.idxmax()\n",
        "print(f\"\\n3. Hardest attack to defend: {hardest} (avg ASR: {type_avg[hardest]:.1%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
